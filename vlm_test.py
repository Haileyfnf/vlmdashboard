"""
Gemini VLM(Vision Language Model) ì´ë¯¸ì§€ ë¶„ì„ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ê¸°ëŠ¥:
1. ê¸°ì¤€ì •ë³´ ì—‘ì…€ íŒŒì‹± (ë§ˆì¼€íŒ… ê¸°ì¤€ì •ë³´ + ì˜ë¥˜ ì¹´í…Œê³ ë¦¬)
2. ì´ë¯¸ì§€ ë¶„ì„ (Gemini Pro Vision)
3. ê²°ê³¼ë¥¼ ì—‘ì…€ë¡œ ì¶œë ¥ (Cat, Subcat, Key, Value í˜•ì‹)
"""
import os
import json
import time
from pathlib import Path
from datetime import datetime
from io import BytesIO
from google import genai
from google.genai import types
import pandas as pd
from PIL import Image as PILImage
from openpyxl.drawing.image import Image as XLImage
from dotenv import load_dotenv

# ê¸°ì¤€ì •ë³´ Python íŒŒì¼ì—ì„œ import
from category_attributes import (
    COMMON_ATTRIBUTES,
    CATEGORY_ATTRIBUTES_MAP,
    BACKGROUND_ATTRIBUTES,
    STYLING_ATTRIBUTES,
    MODEL_ATTRIBUTES
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent
IMAGES_DIR = BASE_DIR  # ë£¨íŠ¸ í´ë”ì˜ ì´ë¯¸ì§€ ë¶„ì„
DATA_DIR = BASE_DIR / "data"
OUTPUT_DIR = BASE_DIR / "output"
REFERENCE_DIR = BASE_DIR / "reference"

# ê¸°ì¤€ì •ë³´ ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
MARKETING_REF_FILE = BASE_DIR / "ë§ˆì¼€íŒ… ê¸°ì¤€ì •ë³´.xlsx"
CATEGORY_REF_FILE = BASE_DIR / "F&F_odd key_values_ver.02_251201.xlsx"


def setup_gemini():
    """Gemini API ì„¤ì •"""
    api_key = os.getenv('GOOGLE_API_KEY')
    
    if not api_key:
        raise ValueError(
            "GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
            ".env íŒŒì¼ì— ë‹¤ìŒì„ ì¶”ê°€í•˜ì„¸ìš”:\n"
            "GOOGLE_API_KEY=your_api_key_here\n\n"
            "API í‚¤ ë°œê¸‰: https://aistudio.google.com/app/apikey"
        )
    
    # ìƒˆë¡œìš´ SDK ë°©ì‹
    client = genai.Client(api_key=api_key)
    return client


# =============================================================================
# ê¸°ì¤€ì •ë³´ íŒŒì‹± í•¨ìˆ˜ë“¤
# =============================================================================

def parse_marketing_reference(file_path: str = None) -> dict:
    """ë§ˆì¼€íŒ… ê¸°ì¤€ì •ë³´ ì—‘ì…€ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    file_path = file_path or MARKETING_REF_FILE
    
    if not Path(file_path).exists():
        print(f"âš ï¸ ë§ˆì¼€íŒ… ê¸°ì¤€ì •ë³´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return {}
    
    df = pd.read_excel(file_path, header=None)
    reference = {}
    
    for col_idx in range(1, len(df.columns)):
        col_data = df.iloc[:, col_idx].dropna().tolist()
        
        if len(col_data) >= 2:
            key_name = str(col_data[0]).strip()
            values = [str(v).strip() for v in col_data[1:] if pd.notna(v) and str(v).strip()]
            
            if key_name and not key_name[0].isascii():
                reference[key_name] = values
    
    print(f"âœ“ ë§ˆì¼€íŒ… ê¸°ì¤€ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(reference)}ê°œ ì¹´í…Œê³ ë¦¬")
    return reference


def parse_category_reference(file_path: str = None) -> dict:
    """ì˜ë¥˜ ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ì •ë³´ ì—‘ì…€ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    file_path = file_path or CATEGORY_REF_FILE
    
    if not Path(file_path).exists():
        print(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ì •ë³´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return {}
    
    df = pd.read_excel(file_path)
    
    reference = {}
    for cat in df['cat'].unique():
        sub_cats = df[df['cat'] == cat]['sub_cat'].dropna().tolist()
        reference[cat] = sub_cats
    
    print(f"âœ“ ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ì •ë³´ ë¡œë“œ ì™„ë£Œ: {len(reference)}ê°œ ëŒ€ë¶„ë¥˜")
    return reference


def load_all_references() -> dict:
    """ëª¨ë“  ê¸°ì¤€ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    return {
        "marketing": parse_marketing_reference(),
        "category": parse_category_reference()
    }


def save_references_to_json(references: dict, output_path: str = None):
    """íŒŒì‹±ëœ ê¸°ì¤€ì •ë³´ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    REFERENCE_DIR.mkdir(exist_ok=True)
    output_path = output_path or (REFERENCE_DIR / "parsed_references.json")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(references, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ ê¸°ì¤€ì •ë³´ JSON ì €ì¥ ì™„ë£Œ: {output_path}")
    return output_path


# =============================================================================
# ì´ë¯¸ì§€ ë¶„ì„ í•¨ìˆ˜ë“¤
# =============================================================================

def encode_image(image_path: str) -> bytes:
    """ì´ë¯¸ì§€ë¥¼ ë°”ì´íŠ¸ë¡œ ì½ìŠµë‹ˆë‹¤."""
    with open(image_path, 'rb') as f:
        return f.read()


def analyze_image(client, image_path: str, analysis_prompt: str) -> dict:
    """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    try:
        image_data = encode_image(image_path)
        
        # ìƒˆë¡œìš´ SDK ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ íŒŒíŠ¸ ìƒì„±
        image_part = types.Part.from_bytes(
            data=image_data,
            mime_type="image/jpeg"
        )
        
        response = client.models.generate_content(
            model="gemini-3-pro-preview",
            contents=[analysis_prompt, image_part]
        )
        
        return {
            "success": True,
            "result": response.text,
            "image_path": str(image_path)
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "image_path": str(image_path)
        }


def create_analysis_prompt(references: dict = None) -> str:
    """ëª…ì‹œì ìœ¼ë¡œ ëª¨ë“  ì†ì„±ì„ ë‚˜ì—´í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì¸ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    prompt = """Analyze this fashion image. Extract ALL visible clothing items separately.

## OUTPUT FORMAT (JSON only, English values):
{
    "Clothing": [
        {
            "cat": "Category name",
            "subcat": "Item type",
            "brand": "Brand if visible, else empty",
            "product_name": "Product name if identifiable, else empty",
            "attributes": { /* category-specific attributes below */ }
        }
    ],
    "Marketing": {
        "age group": "",
        "color tone filter": "",
        "coordination method": "",
        "gender": "",
        "skin tone": "",
        "pose": "",
        "hair style": "",
        "expression": "",
        "gaze direction": "",
        "fashion style": "",
        "location": "",
        "mood": "",
        "number of people": "",
        "overall fashion color tone": "",
        "season weather": "",
        "shooting composition": ""
    }
}

## MARKETING (MUST fill ALL 16 fields - multiple values allowed with comma):
- "age group": child/teenager/youth/adult/middle-aged/elderly
- "color tone filter": reddish/yellowish/blueish/neutral/contrast/monochrome
- "coordination method": layered/tone-on-tone/set-up/mix & match/low-rise/oversized (e.g., "layered, mix & match")
- "gender": male/female
- "skin tone": cool/warm/neutral
- "pose": full body shot/sitting/walking/looking back/aerial shot/exercise/low angle shot (e.g., "sitting, full body shot")
- "hair style": short hair/wave/straight hair/braided/ponytail/pigtails/bangs/crew cut/dyed hair/layered cut/high bun/low bun (e.g., "short hair, bangs")
- "expression": smile/expressionless/surprised/cool/wink
- "gaze direction": front/side/upward/downward/avoiding gaze
- "fashion style": casual/street/business/formal/sporty/luxury/feminine/gorpcore/workwear/y2k/old money look/preppy/bodycon (e.g., "casual, street")
- "location": street/cafÃ©/shopping-store/park-nature/beach/gym/festival/party/city/campus/car/stadium/flight/outdoor-exercise/travel/pool/home/studio (e.g., "street, city")
- "mood": relaxed/active/chic/luxurious/hip/lovely/festive/rebellious/romantic (e.g., "relaxed, chic")
- "number of people": single/couple/group
- "overall fashion color tone": warm tone/cool tone/neutral tone/vivid/pastel/dark/bright
- "season weather": spring/summer/fall/winter
- "shooting composition": full body/upper body/close-up/side view/back view/mid shot

## COMMON ATTRIBUTES (apply to ALL clothing items):
- "color": [array of colors] red/blue/white/black/navy/gray/beige/brown/green/yellow/pink/orange/purple/burgundy/cream/ivory/khaki/olive/coral/mint/sky blue/etc
- "color coordination": multi color/single color/two tone/gradient/color block
- "fabrication": jersey knit/woven/denim/leather/cotton/polyester/wool/silk/linen/velvet/satin/chiffon/lace/mesh/fleece/corduroy/tweed/cashmere/nylon
- "pattern": solid/stripe/check/plaid/floral/polka dot/animal print/leopard pattern/camouflage/graphic/logo/abstract/geometric/paisley/houndstooth/gingham/tartan/tie-dye

## INNER ATTRIBUTES (cat="Inner"):
- "inner neckline": turtle neck/cowl neck/mock neck/crew neck/round neck/scoop neck/v neck/square neck/halter neck/shirt collar/hood/polo collar/boat neck
- "inner front detail": eyelet/lace/wrap/cut-out/twisted/ruched/drape/ribbon/shirring/belt/buckle/zipper/button/piping/stitching/embroidery/applique/patchwork/ruffle
- "inner fabric sheerness": opaque/sheer/slightly-sheer
- "sleeve length": extra-long sleeves/long sleeves/three-quarter sleeves/short sleeves/cap sleeves/sleeveless
- "inner silhouette": slim fit/regular fit/oversize fit
- "inner length": cropped length/waist length/hip length/mid thigh length

## OUTER ATTRIBUTES (cat="Outer"):
- "outer neckline": notched collar/peak-lapel/shirt collar/shawl collar/mandarin collar/wide collar/fur neck/collarless/hood
- "outer front closure": open front/toggle closure/velcro closure/full zip up/half zip up/pullover/double breasted/single breasted/snap button
- "sleeve length": extra long sleeves/long sleeves/three-quarter sleeves/short sleeves/sleeveless
- "outer silhouette": slim fit/regular fit/boxy fit/oversize fit
- "outer length": cropped length/waist length/hip length/mid thigh length/knee-length/calf length/ankle-length

## BOTTOM ATTRIBUTES (cat="Bottom"):
- "pants silhouette": skinny/slim/straight/bootcut/flare/wide/tapered/baggy/fitted
- "skirts silhouette": a-line/h-line/pencil/flared/pleated/tiered/wrap/mermaid
- "pants length": capri length/cropped length/ankle length/top of shoe
- "skirts length": mini length/above knee length/knee length/mid length/maxi length
- "bottoms front detail": pocket/flap pocket/welt pocket/button/zipper/belt/buckle/drawstring/pleats/ruffle
- "waist line": low rise/mid rise/high rise

## BAG ATTRIBUTES (cat="Bag"):
- "bags size": micro/small/medium/large/oversized
- "bags closure type": zip-top/zip-around/magnetic-flap/buckle-flap/turn-lock/drawstring/snap-button/open-top
- "bags detail": ring/d-ring/buckle/zipper/chain/bag charm/metal stud/clasp/quilting/embroidery
- "bags handle and strap type": backpack strap/top handle/double handle/single shoulder strap/cross body/wristlet

## SHOES ATTRIBUTES (cat="Shoes"):
- "heel height": high heel/low heel/mid heel/flat
- "shaft height": high top/knee high/low top/mid calf/mid top/thigh high
- "outsole feature": grip sole/chunky sole/flat sole/platform sole
- "toe shape": almond toe/pointed toe/round toe/square toe
- "material": leather/suede/canvas/mesh/synthetic/patent leather
- "finish": glossy/matte/metallic

## ONEPIECE ATTRIBUTES (cat="Onepiece"):
- "onepiece upper neckline": turtle neck/cowl neck/mock neck/crew neck/round neck/scoop neck/v neck/square neck/sweetheart neck/halter neck/shirt collar/hood/collarless/off the shoulder/one shoulder
- "onepiece strap style": strapless/asymmetric-strap/spaghetti-strap/adjustable-strap/halter-strap
- "sleeve length": extra-long sleeves/long sleeves/three-quarter sleeves/short sleeves/cap sleeves/sleeveless
- "onepiece sleeve type": dolman sleeves/bell sleeves/flutter sleeves/bishop sleeves/puff sleeves/balloon sleeves
- "onepiece fabric sheerness": opaque/slightly-sheer/partial-sheer/ultra-sheer
- "onepiece front detail": wrap/cut-out/twist/ruched/lace/belt/buckle/zipper/button/pleats/embroidery/sequins
- "onepiece front closure": half zip up/full zip up/tie in/half-button/full-button/wrap-closure
- "onepiece back detail": ruched/lace/keyhole/belt/buckle/zipper/pleats/sequins
- "onepiece back closure": back-zip/wrap-tie-back/lace-up-back
- "onepiece skirt silhouette": a-line/flare/pencil/wrap/tiered/mermaid/asymmetrical
- "onepiece skirt length": micro mini length/mini length/above knee length/knee length/midi length/maxi length
- "onepiece waist type": empire/natural/high-waisted/drop-waist
- "onepiece silhouette": slim fit/regular fit/oversize fit

## HOSIERY ATTRIBUTES (cat="Hosiery"):
- "hosiery cuff": frill-cuff/ribbed cuff/lace-up
- "hosiery sheerness": opaque/partial-sheer/slightly-sheer/ultra-sheer
- "hosiery height": ankle/crew/knee-high/no-show/over-the-knee/thigh-high/pantyhose
- "socks toe coverage": full-toe/open-toe/toe-separation

## SWIMWEAR ONEPIECE ATTRIBUTES (cat="Swimwear Onepiece"):
- "swimwear onepiece upper neckline": v neck/scoop neck/boat neck/square neck/sweetheart neck/halter neck
- "swimwear onepiece sleeve length": sleeveless/short sleeves/long sleeves
- "swimwear onepiece detail": slit/eyelet lace/embroidery/fringe/ruffle/keyhole/twisted/ruching
- "swimwear onepiece front closure": open-front/full-button/half-button
- "swimwear onepiece fabric sheerness": slightly-sheer/opaque
- "swimwear onepiece skirt length": mini length/midi length/maxi length
- "swimwear onepiece strap style": strapless/spaghetti-strap/halter-strap/cross-back strap/adjustable-strap/cut-out
- "swimwear onepiece back closure": open-back/lace-up-back

## SWIMWEAR INNER ATTRIBUTES (cat="Swimwear Inner"):
- "swimwear inner neckline": crew neck/mock neck/scoop neck/v neck/square neck/sweetheart neck/halter neck
- "swimwear inner sleeve length": long sleeves/short sleeves
- "swimwear inner front closure": half zip up/full zip up
- "swimwear inner detail": piping/stitching/ruching/ruffle/twisted/keyhole/cut-out
- "swimwear inner strap style": spaghetti-strap/adjustable-strap/halter-strap/cross-back strap/strapless
- "swimwear inner back closure": lace-up-back

## SWIMWEAR BOTTOMS ATTRIBUTES (cat="Swimwear Bottoms"):
- "swimwear bottom pants silhouette": slim/straight/tapered
- "swimwear bottom pants length": cropped length/ankle length
- "swimwear bottom shorts silhouette": fitted/straight-cut/baggy/bermuda
- "swimwear bottom shorts length": mid thigh length/knee length
- "swimwear bottom skirts silhouette": a-line/flared/wrap
- "swimwear bottom skirts length": mini length/above knee length
- "swimwear bottoms closure": elastic-waist/drawstring-waist/wrap-tie
- "swimwear bottoms waist line": low rise/mid rise/high rise
- "swimwear bottoms detail": pocket/zipper pocket/piping/twisted/ruching/lace-up/ruffle

## HEADWEAR ATTRIBUTES (cat="Headwear"):
- "headwear type": cap/beanie/bucket hat/fedora/beret/visor/headband/bandana/sun hat/baseball cap/snapback

## EYEWEAR ATTRIBUTES (cat="Eyewear"):
- "eyewear type": sunglasses/glasses/goggles/reading glasses
- "frame shape": round/square/aviator/cat-eye/rectangular/oval/oversized

## NECKWEAR ATTRIBUTES (cat="Neckwear"):
- "neckwear type": scarf/necktie/bow tie/choker/bandana/neckerchief

## RULES:
1. Analyze EACH visible item separately
2. MUST fill ALL 16 Marketing fields
3. MUST fill ALL common attributes (color, color coordination, fabrication, pattern) for each clothing item
4. Use category-specific attributes based on item type
5. Values in English only. Empty string "" if unknown
6. MULTI-VALUE ALLOWED: If multiple values apply, use comma-separated format (e.g., "sitting, full body shot", "smile, wink", "street, casual")
7. color should be an array if multiple colors visible (e.g., ["red", "white", "blue"])
"""
    return prompt


def parse_gemini_response(response_text: str) -> dict:
    """Gemini ì‘ë‹µì—ì„œ JSONì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    try:
        if "```json" in response_text:
            json_str = response_text.split("```json")[1].split("```")[0]
        elif "```" in response_text:
            json_str = response_text.split("```")[1].split("```")[0]
        else:
            json_str = response_text
        
        return json.loads(json_str.strip())
    except json.JSONDecodeError:
        return {"raw_response": response_text}


def flatten_to_vertical(parsed_data: dict, image_name: str) -> list:
    """
    JSONì„ Cat, Subcat, Key, Value í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    rows = []
    
    if "raw_response" in parsed_data:
        rows.append({
            "Image": image_name,
            "Cat": "Error",
            "Subcat": "Parse Failed",
            "Key": "Raw Response",
            "Value": parsed_data["raw_response"][:500]
        })
        return rows
    
    # Unknown, None ê°’ì„ ë¹ˆ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    def clean_value(v):
        if v is None:
            return ""
        v_str = str(v).strip()
        if v_str.lower() in ["unknown", "none", "n/a", "null", "undefined"]:
            return ""
        return v_str
    
    # ì˜ë¥˜ ì •ë³´ ì²˜ë¦¬ (Clothing) - ë°°ì—´ë¡œ ì—¬ëŸ¬ ì•„ì´í…œ ì²˜ë¦¬
    clothing_list = parsed_data.get("Clothing", [])
    
    # ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš° ë°°ì—´ë¡œ ë³€í™˜
    if isinstance(clothing_list, dict):
        clothing_list = [clothing_list]
    
    for clothing in clothing_list:
        cat = clean_value(clothing.get("cat", ""))
        subcat = clean_value(clothing.get("subcat", ""))
        brand = clean_value(clothing.get("brand", ""))
        product_name = clean_value(clothing.get("product_name", ""))
        attributes = clothing.get("attributes", {})
        
        first_row = True
        
        # ë¸Œëœë“œ ì •ë³´ ì¶”ê°€
        if brand:
            rows.append({
                "Image": image_name,
                "Cat": cat if first_row else "",
                "Subcat": subcat if first_row else "",
                "Key": "brand",
                "Value": brand
            })
            first_row = False
        
        # ì œí’ˆëª… ì •ë³´ ì¶”ê°€
        if product_name:
            rows.append({
                "Image": image_name,
                "Cat": cat if first_row else "",
                "Subcat": subcat if first_row else "",
                "Key": "product_name",
                "Value": product_name
            })
            first_row = False
        
        for key, value in attributes.items():
            if isinstance(value, list):
                # ë°°ì—´ì¸ ê²½ìš° ì‰¼í‘œë¡œ í•©ì³ì„œ í•œ í–‰ìœ¼ë¡œ
                cleaned_values = [clean_value(v) for v in value if clean_value(v)]
                if cleaned_values:
                    rows.append({
                        "Image": image_name,
                        "Cat": cat if first_row else "",
                        "Subcat": subcat if first_row else "",
                        "Key": key,
                        "Value": ", ".join(cleaned_values)
                    })
                    first_row = False
            else:
                cleaned = clean_value(value)
                if cleaned:  # ë¹ˆ ê°’ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                    rows.append({
                        "Image": image_name,
                        "Cat": cat if first_row else "",
                        "Subcat": subcat if first_row else "",
                        "Key": key,
                        "Value": cleaned
                    })
                    first_row = False
        
        # ì†ì„±ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ cat, subcatë§Œì´ë¼ë„ ì¶”ê°€
        if first_row and (cat or subcat):
            rows.append({
                "Image": image_name,
                "Cat": cat,
                "Subcat": subcat,
                "Key": "",
                "Value": ""
            })
    
    # ë§ˆì¼€íŒ… ì†ì„± ì²˜ë¦¬ (Marketing)
    marketing = parsed_data.get("Marketing", {})
    for attr_name, attr_value in marketing.items():
        cleaned = clean_value(attr_value)
        if cleaned:  # ë¹ˆ ê°’ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
            rows.append({
                "Image": image_name,
                "Cat": attr_name,
                "Subcat": "",
                "Key": "",
                "Value": cleaned
            })
    
    return rows


# =============================================================================
# ë°°ì¹˜ ë¶„ì„ ë° ì €ì¥
# =============================================================================

def analyze_images_batch(
    image_paths: list,
    output_excel: str = None,
    references: dict = None,
    batch_size: int = 1  # 1ì¥ì”© ë¶„ì„ (ì •í™•ë„ ìµœìš°ì„ )
) -> pd.DataFrame:
    """ì´ë¯¸ì§€ë¥¼ ê°œë³„ ë¶„ì„í•©ë‹ˆë‹¤. (ì •í™•ë„ ìµœìš°ì„ )"""
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    print("ğŸ”§ Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
    client = setup_gemini()
    
    # ë°°ì¹˜ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì • (ì—¬ëŸ¬ ì¥ì„ ì²˜ë¦¬í•˜ë¼ê³  ì§€ì‹œ)
    base_prompt = create_analysis_prompt(references)
    batch_instruction = """
    
    ## BATCH ANALYSIS INSTRUCTION:
    - You will receive multiple images.
    - Analyze EACH image sequentially.
    - Return a JSON Object with a key "results" containing a list of analysis for each image.
    - The order of the list must match the order of images provided.
    
    Example Output Structure:
    {
        "results": [
            { "file_name": "image1.jpg", "analysis": { ... analysis for image 1 ... } },
            { "file_name": "image2.jpg", "analysis": { ... analysis for image 2 ... } }
        ]
    }
    """
    final_prompt = base_prompt + batch_instruction

    all_rows = []
    total = len(image_paths)
    
    # ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ ì‚¬ì´ì¦ˆë§Œí¼ ìë¥´ê¸° (chunking)
    chunks = [image_paths[i:i + batch_size] for i in range(0, total, batch_size)]
    
    print(f"\nğŸ“Š ì´ {total}ê°œ ì´ë¯¸ì§€, {len(chunks)}ê°œ ë°°ì¹˜ë¡œ ë¶„ì„ ì‹œì‘ (ë°°ì¹˜í¬ê¸°: {batch_size})...\n")
    
    total_start_time = time.time()
    
    for batch_idx, chunk in enumerate(chunks, 1):
        print(f"ğŸ“¦ ë°°ì¹˜ [{batch_idx}/{len(chunks)}] ì²˜ë¦¬ ì¤‘ ({len(chunk)}ì¥)...")
        
        try:
            # 1. ì´ë²ˆ ë°°ì¹˜ì˜ ì´ë¯¸ì§€ ë°ì´í„°ë“¤ ì¤€ë¹„
            contents = [final_prompt] # í”„ë¡¬í”„íŠ¸ ë¨¼ì € ë„£ê³ 
            batch_files = [] # íŒŒì¼ëª… ë§¤í•‘ìš©
            
            for img_path in chunk:
                img_data = encode_image(img_path)
                image_part = types.Part.from_bytes(data=img_data, mime_type="image/jpeg")
                contents.append(image_part) # ì´ë¯¸ì§€ ê³„ì† ì¶”ê°€
                batch_files.append(Path(img_path).name)

            # 2. API í•œ ë²ˆ í˜¸ì¶œë¡œ ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ë¶„ì„
            response = client.models.generate_content(
                model="gemini-3-pro-preview",
                contents=contents,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json" # JSON ê°•ì œ ëª¨ë“œ (í† í° ì ˆì•½)
                )
            )
            
            # 3. ê²°ê³¼ íŒŒì‹± ë° ë§¤í•‘ (JSON ì •ì œ - Extra data ì—ëŸ¬ ë°©ì§€)
            response_text = response.text.strip()
            
            # JSON ì™¸ í…ìŠ¤íŠ¸ ì œê±° (```json ... ``` í˜•íƒœ ì²˜ë¦¬)
            if "```json" in response_text:
                response_text = response_text.split("```json")[1].split("```")[0]
            elif "```" in response_text:
                response_text = response_text.split("```")[1].split("```")[0]
            
            # ì²« ë²ˆì§¸ ìœ íš¨í•œ JSON ê°ì²´ë§Œ ì¶”ì¶œ (raw_decode ì‚¬ìš©)
            response_text = response_text.strip()
            try:
                # raw_decodeëŠ” ì²« ë²ˆì§¸ ì™„ì „í•œ JSONë§Œ íŒŒì‹±í•˜ê³  ë‚˜ë¨¸ì§€ ë¬´ì‹œ
                decoder = json.JSONDecoder()
                result_json, _ = decoder.raw_decode(response_text)
            except json.JSONDecodeError:
                # { ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
                start_idx = response_text.find('{')
                if start_idx != -1:
                    result_json, _ = decoder.raw_decode(response_text[start_idx:])
                else:
                    raise
            
            results_list = result_json.get("results", [])
            
            # ê°œìˆ˜ ë¶ˆì¼ì¹˜ ì•ˆì „ì¥ì¹˜: ê²°ê³¼ê°€ ì´ë¯¸ì§€ ìˆ˜ë³´ë‹¤ ì ì„ ê²½ìš° ëŒ€ë¹„
            if isinstance(result_json, list): # í˜¹ì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ë¡œ ì¤„ ê²½ìš°
                results_list = result_json
            
            # 4. ê° ì´ë¯¸ì§€ë³„ ê²°ê³¼ ì €ì¥
            for i, img_name in enumerate(batch_files):
                if i < len(results_list):
                    # êµ¬ì¡°ì— ë”°ë¼ ë¶„ì„ ë°ì´í„° ì¶”ì¶œ
                    analysis_data = results_list[i].get("analysis", results_list[i])
                    # ê¸°ì¡´ flatten í•¨ìˆ˜ ì¬ì‚¬ìš©
                    rows = flatten_to_vertical(analysis_data, img_name)
                    all_rows.extend(rows)
                else:
                    # ëˆ„ë½ëœ ê²½ìš°
                    all_rows.append({"Image": img_name, "Cat": "Error", "Key": "Batch Error", "Value": "Missing in response"})

            print(f"  âœ“ ë°°ì¹˜ ì™„ë£Œ")
            
        except Exception as e:
            print(f"  âŒ ë°°ì¹˜ ì‹¤íŒ¨: {str(e)}")
            for img_path in chunk:
                all_rows.append({
                    "Image": Path(img_path).name,
                    "Cat": "Error",
                    "Subcat": "",
                    "Key": "Exception",
                    "Value": str(e)
                })
    
    # ì†Œìš” ì‹œê°„ ê³„ì‚°
    total_elapsed = time.time() - total_start_time
    
    # DataFrame ìƒì„±
    df = pd.DataFrame(all_rows, columns=["Image", "Cat", "Subcat", "Key", "Value"])
    
    # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸
    success_count = len([r for r in all_rows if r.get("Cat") != "Error"])
    fail_count = len([r for r in all_rows if r.get("Cat") == "Error"])
    
    if output_excel is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_excel = OUTPUT_DIR / f"vlm_analysis_result_{timestamp}.xlsx"
    
    with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:
        df.to_excel(writer, index=False, sheet_name='ë¶„ì„ê²°ê³¼')
        
        worksheet = writer.sheets['ë¶„ì„ê²°ê³¼']
        
        # ì—´ ë„ˆë¹„ ì„¤ì •
        worksheet.column_dimensions['A'].width = 15  # Image
        worksheet.column_dimensions['B'].width = 25  # Cat
        worksheet.column_dimensions['C'].width = 20  # Subcat
        worksheet.column_dimensions['D'].width = 25  # Key
        worksheet.column_dimensions['E'].width = 40  # Value
        
        # ì´ë¯¸ì§€ ì¸ë„¤ì¼ ì‚½ì…
        print("\nğŸ–¼ï¸ ì—‘ì…€ì— ì´ë¯¸ì§€ ì¸ë„¤ì¼ ì‚½ì… ì¤‘...")
        inserted_images = set()
        
        for row_idx, row in enumerate(df.itertuples(), start=2):
            image_name = row.Image
            
            if image_name not in inserted_images:
                image_path = IMAGES_DIR / image_name
                
                if image_path.exists():
                    try:
                        img = PILImage.open(image_path)
                        img.thumbnail((80, 80))
                        
                        img_buffer = BytesIO()
                        img.save(img_buffer, format='JPEG')
                        img_buffer.seek(0)
                        
                        xl_img = XLImage(img_buffer)
                        xl_img.width = 80
                        xl_img.height = 80
                        
                        cell = f'A{row_idx}'
                        worksheet.add_image(xl_img, cell)
                        
                        worksheet.row_dimensions[row_idx].height = 65
                        
                        inserted_images.add(image_name)
                    except Exception as e:
                        print(f"  âš ï¸ ì´ë¯¸ì§€ ì‚½ì… ì‹¤íŒ¨ ({image_name}): {e}")
        
        print(f"  âœ“ {len(inserted_images)}ê°œ ì´ë¯¸ì§€ ì¸ë„¤ì¼ ì‚½ì… ì™„ë£Œ")
    
    avg_time = total_elapsed / total if total > 0 else 0
    
    print(f"\n{'='*60}")
    print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ˆ ì„±ê³µ: {success_count}ê°œ í•­ëª© / ì‹¤íŒ¨: {fail_count}ê°œ í•­ëª©")
    print(f"ğŸ“Š ì´ {len(df)}ê°œ í–‰ ìƒì„±")
    print(f"â±ï¸ ì´ ì†Œìš”ì‹œê°„: {total_elapsed:.1f}ì´ˆ (í‰ê·  {avg_time:.1f}ì´ˆ/ì´ë¯¸ì§€)")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_excel}")
    print(f"{'='*60}\n")
    
    return df


def analyze_single_image(image_path: str, references: dict = None) -> dict:
    """ë‹¨ì¼ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤. (í…ŒìŠ¤íŠ¸ìš©)"""
    client = setup_gemini()
    
    prompt = create_analysis_prompt(references)
    
    print(f"ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì¤‘: {image_path}")
    result = analyze_image(client, image_path, prompt)
    
    if result["success"]:
        parsed = parse_gemini_response(result["result"])
        print("\nğŸ“‹ ë¶„ì„ ê²°ê³¼ (JSON):")
        print(json.dumps(parsed, ensure_ascii=False, indent=2))
        
        rows = flatten_to_vertical(parsed, Path(image_path).name)
        print("\nğŸ“‹ ë¶„ì„ ê²°ê³¼ (í…Œì´ë¸” í˜•ì‹):")
        print("-" * 100)
        print(f"{'Cat':<25} {'Subcat':<20} {'Key':<25} {'Value':<25}")
        print("-" * 100)
        for row in rows:
            val = row['Value'][:25] if len(row['Value']) > 25 else row['Value']
            print(f"{row['Cat']:<25} {row['Subcat']:<20} {row['Key']:<25} {val:<25}")
        
        return parsed
    else:
        print(f"\nâŒ ë¶„ì„ ì‹¤íŒ¨: {result.get('error')}")
        return result


def get_image_list(folder_path: str = None, pattern: str = "*.jpg") -> list:
    """í´ë”ì—ì„œ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    folder = Path(folder_path) if folder_path else IMAGES_DIR
    images = [f for f in folder.glob(pattern) if f.is_file() and not f.name.startswith('.')]
    print(f"ğŸ“ {folder}ì—ì„œ {len(images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
    return sorted(images)


# =============================================================================
# ë©”ì¸ ì‹¤í–‰
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("="*60)
    print("ğŸ–¼ï¸  Gemini VLM ì´ë¯¸ì§€ ë¶„ì„ (ì¹´í…Œê³ ë¦¬ë³„ ì†ì„±)")
    print("="*60)
    
    # ê¸°ì¤€ì •ë³´ íŒŒì‹± ë° ì €ì¥
    print("\n[1] ê¸°ì¤€ì •ë³´ íŒŒì‹± ì¤‘...")
    references = load_all_references()
    save_references_to_json(references)
    
    # ê¸°ì¤€ì •ë³´ ìš”ì•½ ì¶œë ¥
    print("\nğŸ“‹ ë§ˆì¼€íŒ… ê¸°ì¤€ì •ë³´:")
    for key, values in references["marketing"].items():
        print(f"  - {key}: {len(values)}ê°œ ê°’")
    
    print("\nğŸ“‹ ì˜ë¥˜ ì¹´í…Œê³ ë¦¬:")
    for cat, sub_cats in references["category"].items():
        print(f"  - {cat}: {len(sub_cats)}ê°œ ì†Œë¶„ë¥˜")
    
    # ì´ë¯¸ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    test_images = get_image_list()
    
    if test_images:
        # ë‹¨ì¼ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸
        print(f"\n[2] ë‹¨ì¼ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ (ì´ {len(test_images)}ê°œ ì´ë¯¸ì§€ ë°œê²¬)")
        result = analyze_single_image(str(test_images[0]), references)
        
        # ë°°ì¹˜ ë¶„ì„
        if len(test_images) > 1:
            print(f"\n[3] ë°°ì¹˜ ë¶„ì„ ({len(test_images)}ê°œ ì´ë¯¸ì§€)")
            df = analyze_images_batch(
                image_paths=[str(img) for img in test_images],
                references=references
            )
    else:
        print(f"âš ï¸ ë¶„ì„í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. vlm_image í´ë”ì— jpg íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")


if __name__ == "__main__":
    main()
